{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import scipy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDictionary(wordTokens, cutoff=0):\n",
    "    '''\n",
    "    Given a source file it generates a dictionary with a unique token string as a key and the identification integer as a value\n",
    "\n",
    "    Input:\n",
    "    :filePath: path to the source file\n",
    "    :cutoff: optional cutoff value to eliminate words with low frequency, if not initialized ano word is cutoff\n",
    "\n",
    "    Output:\n",
    "    :tokenMapping: a mapping from unique tokens/POS-tag pairing to unique integer identifications\n",
    "    '''\n",
    "\n",
    "    print(\"Getting Word Dictionary...\", end='')\n",
    "    \n",
    "    tokenMapping = {}\n",
    "\n",
    "    # count each token\n",
    "    for t in wordTokens:\n",
    "        tokenMapping[t] = tokenMapping.get(t, 0) + 1\n",
    "\n",
    "    # remove words below cutoff and set value as index\n",
    "    idx = 0\n",
    "    for k, v in tokenMapping.items():\n",
    "        if v < cutoff:\n",
    "            del tokenMapping[k]\n",
    "        else:\n",
    "            tokenMapping[k] = idx\n",
    "            idx += 1\n",
    "    tokenMapping['UNK'] = len(tokenMapping)\n",
    "\n",
    "    print(\" Done\")\n",
    "    return tokenMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTokens(fileName):\n",
    "    '''\n",
    "    Gets list of tokens from file seperated by spaces\n",
    "\n",
    "    Input:\n",
    "    :fileName: path to file\n",
    "\n",
    "    Output:\n",
    "    :tokenMapping: list of word tokens\n",
    "    '''\n",
    "    \n",
    "    print(\"Getting Tokens...\", end='')\n",
    "    \n",
    "    with open(fileName) as f:\n",
    "        textBody = f.read()\n",
    "    \n",
    "    print(\" Done\")\n",
    "    return textBody.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCtxMatrix(tokens, dictionary, window):\n",
    "    ''' \n",
    "    Given an ordered list of tokens, creates a sparse context matrix, returned as a lil_matrix.\n",
    "    \n",
    "    Input:\n",
    "    :tokens: list of ordered word tokens\n",
    "    :dictionary: word to numbered id mapping\n",
    "    :window: the window size to consider when creating context matrix\n",
    "    \n",
    "    Output:\n",
    "    :mat: a sparse context matrix, returned as a lil_matrix\n",
    "    '''\n",
    "    \n",
    "    dim = len(dictionary)\n",
    "    \n",
    "    print(\"Creating Container Matrix of size %d...\" % dim, end='')\n",
    "    \n",
    "    mat = scipy.sparse.lil_matrix((dim, dim), dtype=np.int)\n",
    "    for i in range(dim):\n",
    "        mat[i] = np.zeros(dim, dtype=np.int)\n",
    "    \n",
    "    print(\" Done\")\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    print(\"Building Context Matrix...\", end='')\n",
    "        \n",
    "    for i in range(window, len(tokens) - window):\n",
    "        \n",
    "        lowerBound = i - window\n",
    "        upperBound = i + window\n",
    "\n",
    "        context = tokens[lowerBound:i] + tokens[i + 1:upperBound + 1]\n",
    "        target = tokens[i]\n",
    "        \n",
    "        for c in context:\n",
    "            if target in dictionary and c in dictionary:\n",
    "                mat[dictionary[target], dictionary[c]] += 1\n",
    "                \n",
    "    print(' Finished in: %.2f sec' % (time.time() - start))\n",
    "    \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPmiMatrix(ctxMatrix, cutoff_0=True):\n",
    "    '''\n",
    "    Given a sparse context matrix, returns a sparse matrix with the Pointwise Mutual Information Score\n",
    "    \n",
    "    Input:\n",
    "    :ctxMatrix: context Matrix\n",
    "    :cutoff: optional cutoff flag to eliminate coocurrences that have a score below 0\n",
    "\n",
    "    Output:\n",
    "    :pmiMatrix: PMI Matrix\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    print('Preparing to build PMI Matrix...', end='')\n",
    "    \n",
    "    #step 1: get the probability of each term in the matrix - sum over columns, then sum over sums\n",
    "    wordCounts = ctxMatrix.sum(axis=1)\n",
    "    totalWords = wordCounts.sum()\n",
    "    wordProbas = wordCounts/totalWords\n",
    "    \n",
    "    pmiMatrix = scipy.sparse.lil_matrix(ctxMatrix.shape)\n",
    "    \n",
    "    print(' Done')\n",
    "    \n",
    "    print('Building PMI Matrix...', end='')\n",
    "    \n",
    "    start = time.time()\n",
    "    for i in range(ctxMatrix.shape[1]): #row-wise operation\n",
    "        rowPmi = np.log(np.divide( ((ctxMatrix[i].toarray().T)/totalWords) , wordProbas*wordProbas[i] )).T\n",
    "\n",
    "        if(cutoff_0):\n",
    "            rowPmi[rowPmi<0] = 0 #0 cutoff\n",
    "\n",
    "        pmiMatrix[i, :] = rowPmi\n",
    "        del rowPmi\n",
    "\n",
    "    print(' Finished in: %.2f sec' % (time.time()-start))\n",
    "    \n",
    "    return pmiMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchCreator(filePath, window, batchSize, dictionary, pmiMatrix):\n",
    "\n",
    "    '''\n",
    "    Given an input string (with spaced full stops \" . \" to delimit sentences), creates a sparse context matrix, returned as a lil_matrix.\n",
    "\n",
    "    Input:\n",
    "    :string: input string with spaced full stop to delimit sentences\n",
    "    :num_words: integer indicating the top words to keep. Pass 0 if want to pass predefined index\n",
    "    :window: the window size to consider when creating context matrix\n",
    "    :word_list: if num_words is 0, pass a list of words to generate a context matrix on + a 10 cutoff of rare words.\n",
    "\n",
    "    Output:\n",
    "    :matrix: a sparse context matrix, returned as a lil_matrix\n",
    "    :index: a python dictionary mapping word strings to integers\n",
    "    '''\n",
    "\n",
    "    print('Creating Batches...', end='')\n",
    "    start = time.time()\n",
    "\n",
    "    batches = []\n",
    "    with open(filePath) as f:\n",
    "        currentBatch = np.zeros((2,batchSize), dtype=np.int)\n",
    "        trail = \"\"\n",
    "        batchIndex = 0\n",
    "        for chunk in f:\n",
    "            batches, currentBatch, trail, batchIndex = createBatches(trail + chunk, window, batchSize, batchIndex, batches, currentBatch, dictionary, pmiMatrix)\n",
    "            if(batchIndex > 0):\n",
    "                lastBatch = np.zeros((2,batchIndex), dtype=np.int)\n",
    "                lastBatch[0] = currentBatch[0][:batchIndex]\n",
    "                lastBatch[1] = currentBatch[1][:batchIndex]\n",
    "                batches.append(lastBatch)\n",
    "\n",
    "    print(\"\\nFinished in: %.2f sec\" % (time.time() - start))\n",
    "    return batches\n",
    "\n",
    "\n",
    "def createBatches(chunk, window, batchSize, batchIndex, batches, batch, dictionary, pmiMatrix):\n",
    "\n",
    "    tokenWords = chunk.split()\n",
    "    tokens = list(map(lambda t: dictionary[t] if t in dictionary else dictionary['UNK'], tokenWords))\n",
    "\n",
    "    for i in range(window, len(tokens) - window):\n",
    "\n",
    "        lowerBound = i - window\n",
    "        upperBound = i + window\n",
    "\n",
    "        context = tokens[lowerBound:i] + tokens[i + 1:upperBound + 1]\n",
    "        target = tokens[i]\n",
    "\n",
    "        for c in context:\n",
    "\n",
    "            if pmiMatrix[target, c] > 0:\n",
    "                batch[0][batchIndex] = target\n",
    "                batch[1][batchIndex] = c\n",
    "                batchIndex += 1\n",
    "\n",
    "                if batchIndex >= batchSize:\n",
    "                    print(\".\", end='')\n",
    "                    batches.append(batch)\n",
    "                    batchIndex = 0\n",
    "                    batch = np.zeros((2,batchSize), dtype=np.int)\n",
    "\n",
    "    trail = \" \".join(tokenWords[len(tokens) - window:])\n",
    "    return (batches, batch, trail, batchIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Tokens... Done\n",
      "Getting Word Dictionary... Done\n",
      "Creating Container Matrix of size 349... Done\n",
      "Building Context Matrix... Finished in: 0.39 sec\n",
      "Preparing to build PMI Matrix... Done\n",
      "Building PMI Matrix... Finished in: 0.17 sec\n",
      "Creating Batches....................."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samyzarour/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:29: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samyzarour/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samyzarour/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:29: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/samyzarour/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................\n",
      "Finished in: 0.11 sec\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def main(fileName, window, batchSize):\n",
    "    tokens = getTokens(fileName)\n",
    "    dictionary = makeDictionary(tokens)\n",
    "    ctxMatrix = getCtxMatrix(tokens, dictionary, window)\n",
    "    pmiMatrix = getPmiMatrix(ctxMatrix)\n",
    "    batches = batchCreator(fileName, window, 100, dictionary, pmiMatrix)\n",
    "    return batches\n",
    "\n",
    "fileName = '../data/test'\n",
    "window = 10\n",
    "batchSize = 1000\n",
    "\n",
    "main(fileName, window, batchSize)\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
